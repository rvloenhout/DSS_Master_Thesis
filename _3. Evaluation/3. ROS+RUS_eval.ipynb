{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install plot-metric\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "path_csv = \"../../../Thesis_data/processed_data/\"\n",
    "path_model = \"../../../Thesis_data/Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mikulskibartosz.name/how-to-reduce-memory-usage-in-pandas\n",
    "\n",
    "def rm(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"Model\", \"AUROC\", \"Balanced Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
    "\n",
    "def evaluation(y_test_input, y_pred_input, model_name, y_pred_proba_input):\n",
    "\n",
    "    auroc = round(roc_auc_score(y_test_input, y_pred_proba_input),3)\n",
    "    bal_acc = round(balanced_accuracy_score(y_test_input, y_pred_input),3)\n",
    "    f1 = round(f1_score(y_test_input, y_pred_input),3)\n",
    "    recall = round(recall_score(y_test_input, y_pred_input),3)\n",
    "    precis = round(precision_score(y_test_input, y_pred_input),3)\n",
    "\n",
    "    results.loc[len(results)+1] = [type(model_name).__name__, auroc, bal_acc, f1, recall, precis]\n",
    "\n",
    "    print(\"AUROC Score: \", auroc)\n",
    "    print(\"Balanced Accuracy Score: \", bal_acc)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Recall Score: \", recall)\n",
    "    print(\"Precision Score: \", precis)\n",
    "    cm = confusion_matrix(y_test_input, y_pred_input)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 6))\n",
    "    ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(path_csv + \"ontime_reporting_X_test.csv\")\n",
    "y_test = pd.read_csv(path_csv + \"ontime_reporting_y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = rm(X_test)\n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Logistic Regression tuned ROS+RUS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_tuned_rosrus = joblib.load(path_model + \"logreg_tuned_ROSRUS.joblib\")\n",
    "y_pred_logreg_tuned_rosrus = logreg_tuned_rosrus.predict(X_test)\n",
    "y_pred_prob_logreg_tuned_rosrus = logreg_tuned_rosrus.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_test, y_pred_logreg_tuned_rosrus, logreg_tuned_rosrus, y_pred_prob_logreg_tuned_rosrus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate XGBoost tuned ROS+RUS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_class_tuned_rosrus = joblib.load(path_model + \"XGBoost_tuned_ROSRUS.joblib\")\n",
    "y_pred_xgb_tuned_rosrus = xgb_class_tuned_rosrus.predict(X_test)\n",
    "y_pred_prob_xgb_tuned_rosrus = xgb_class_tuned_rosrus.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_test, y_pred_xgb_tuned_rosrus, xgb_class_tuned_rosrus, y_pred_prob_xgb_tuned_rosrus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TabNet tuned ROS+RUS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_tuned_rosrus = torch.load(path_model + \"TabNet_tuned_ROSRUS.pt\", map_location=torch.device('cpu'))\n",
    "tabnet_tuned_rosrus.device = \"cpu\"\n",
    "y_pred_tabnet_tuned_rosrus = tabnet_tuned_rosrus.predict(X_test.values)\n",
    "y_pred_prob_tabnet_tuned_rosrus = tabnet_tuned_rosrus.predict_proba(X_test.values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_test, y_pred_tabnet_tuned_rosrus, tabnet_tuned_rosrus, y_pred_prob_tabnet_tuned_rosrus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Random Forests tuned ROS+RUS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_rosrus = joblib.load(path_model + \"rf_tuned_ROSRUS.joblib\")\n",
    "y_pred_rf_tuned_rosrus = rf_tuned_rosrus.predict(X_test)\n",
    "y_pred_prob_rf_tuned_rosrus = rf_tuned_rosrus.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_test, y_pred_rf_tuned_rosrus, rf_tuned_rosrus, y_pred_prob_rf_tuned_rosrus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=[\"AUROC\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot AUROC for LogReg\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob_logreg_tuned_rosrus)\n",
    "auc = round(roc_auc_score(y_test, y_pred_prob_logreg_tuned_rosrus), 3)\n",
    "plt.plot(fpr, tpr, linestyle='--', label=\"Logistic Regression, AUC=\"+str(auc))\n",
    "\n",
    "#Plot AUROC for XGBoost\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob_xgb_tuned_rosrus)\n",
    "auc = round(roc_auc_score(y_test, y_pred_prob_xgb_tuned_rosrus), 3)\n",
    "plt.plot(fpr, tpr, linestyle='--', label=\"Gradient Boosting, AUC=\"+str(auc))\n",
    "\n",
    "#Plot AUROC for TabNet\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob_tabnet_tuned_rosrus)\n",
    "auc = round(roc_auc_score(y_test, y_pred_prob_tabnet_tuned_rosrus), 3)\n",
    "plt.plot(fpr, tpr, linestyle='--', label=\"TabNet, AUC=\"+str(auc))\n",
    "\n",
    "#Plot AUROC for Random Forests\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob_rf_tuned_rosrus)\n",
    "auc = round(roc_auc_score(y_test, y_pred_prob_rf_tuned_rosrus), 3)\n",
    "plt.plot(fpr, tpr, linestyle='--', label=\"Random Forests, AUC=\"+str(auc))\n",
    "\n",
    "#Add baseline of 0.5 random guesser\n",
    "plt.plot([0, 1], [0, 1], color='gray')\n",
    "\n",
    "#Add legend\n",
    "plt.legend()\n",
    "\n",
    "#Add labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 12)) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
