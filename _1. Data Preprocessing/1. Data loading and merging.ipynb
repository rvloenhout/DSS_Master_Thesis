{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_import = \"../../../Thesis_data/raw_data/\"\n",
    "path_export = \"../../../Thesis_data/processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pre-selected columns from all datasets\n",
    "cols_ontime_reporting = [\"MONTH\", #month of departure, 1 = January, ... 12 = December\n",
    "                         \"DAY_OF_MONTH\", #Day of month of departure\n",
    "                         \"DAY_OF_WEEK\", #Day of week of departure, 1 = Monday, ... 7 = Sunday\n",
    "                         \"TAIL_NUM\", #Unique tail number of aircraft\n",
    "                         \"ORIGIN_AIRPORT_ID\", #Unique airport id, matches with ORIGIN\n",
    "                         \"ORIGIN\", #International Air Transport Association's (IATA) Location Identifier code, unique 3 letter code matches to departure location\n",
    "                         \"ORIGIN_CITY_NAME\", #City name with state abbreviation used to match with us_cities 'City'\n",
    "                         \"DEST\", #International Air Transport Association's (IATA) Location Identifier code, unique 3 letter code matches to destination location\n",
    "                         \"DISTANCE_GROUP\", #Miles between ORIGIN and DESTINATION, grouped together by integers, see below this cell for a detailed description\n",
    "                         \"CRS_DEP_TIME\", #4 digit military time formatting of the planned departure time\n",
    "                         \"CRS_ARR_TIME\", #4 digit military time formatting of the planned arrival time\n",
    "                         \"DEP_DEL15\", #Binary number that classifies a delay (1) as: a aircraft departing 15 minutes later than planned\n",
    "                         \"OP_UNIQUE_CARRIER\" #Categorical variable used to identify the carrier, an additional .CSV is provided to correlate these to full names\n",
    "                         ]\n",
    "\n",
    "cols_aircraft_inventory = [\"TAIL_NUM\", #Unique tail number of aircraft\n",
    "                           \"MANUFACTURE_YEAR\", #Manufacturing year of the plane\n",
    "                           \"NUMBER_OF_SEATS\" #N of seats on a plane\n",
    "                           ]\n",
    "\n",
    "cols_airport_list = [\"ORIGIN_AIRPORT_ID\", #Unique airport id, matches with ORIGIN_AIRPORT_ID from ontime_reporting\n",
    "                     \"NAME\" #Location of weather reading, matches with NAME from airport_weather\n",
    "                     ]\n",
    "\n",
    "cols_airport_weather = [\"NAME\", #Location of weather reading\n",
    "                        \"DATE\", #Date in month/day/year format\n",
    "                        \"PRCP\", #Preciptation that day in inches\n",
    "                        \"SNOW\", #Snowfall that day in inches\n",
    "                        \"SNWD\", #Depth of snow that day in inches\n",
    "                        \"TMAX\", #Maximum temperature that day in Fahrenheit\n",
    "                        \"AWND\" #Maximum wind speed that day in Miles per Hour\n",
    "                        ]\n",
    "\n",
    "cols_us_cities = [\"City\", #City name with state abbreviation used to match with ontime_reporting ORIGIN_CITY_NAME\n",
    "                  \"Median Age\",\n",
    "                  \"Total Population\",\n",
    "                  \"Average Household Size\"\n",
    "                  ]\n",
    "\n",
    "cols_airport_geolocation = [\"iata\", #International Air Transport Association's (IATA) Location Identifier code, unique 3 letter code\n",
    "                            \"country_code\", #2 letter code that represents the country\n",
    "                            \"latitude\",\n",
    "                            \"longitude\"]\n",
    "\n",
    "#DISTANCE_GROUPS:\n",
    "#1,\"Less Than 250 Miles\"\n",
    "#2,\"250-499 Miles\"\n",
    "#3,\"500-749 Miles\"\n",
    "#4,\"750-999 Miles\"\n",
    "#5,\"1000-1249 Miles\"\n",
    "#6,\"1250-1499 Miles\"\n",
    "#7,\"1500-1749 Miles\"\n",
    "#8,\"1750-1999 Miles\"\n",
    "#9,\"2000-2249 Miles\"\n",
    "#10,\"2250-2499 Miles\"\n",
    "#11,\"2500 Miles and Greater\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading on-time reporting data for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583985, 13)\n",
      "(1117160, 13)\n",
      "(1749234, 13)\n",
      "(2361257, 13)\n",
      "(2997647, 13)\n",
      "(3634338, 13)\n",
      "(4293367, 13)\n",
      "(4951828, 13)\n",
      "(5557807, 13)\n",
      "(6193821, 13)\n",
      "(6796274, 13)\n",
      "(7422037, 13)\n"
     ]
    }
   ],
   "source": [
    "#Loading the ontime_reporting data for each month and concatanating them on an empty DataFrame\n",
    "ontime_reporting_all = pd.DataFrame() #Define empty dataframe\n",
    "\n",
    "#Going through all the CSV files (12, for each month one) related to On-Time Airplane Reporting and concatenating them\n",
    "for i in range(1,13): #13\n",
    "    if i == 1:\n",
    "        ontime_reporting_montly = pd.read_csv(path_import + \"ONTIME_REPORTING_\" + str(i) + \".csv\", usecols=cols_ontime_reporting)\n",
    "        ontime_reporting_all = ontime_reporting_montly\n",
    "        print(ontime_reporting_all.shape)\n",
    "    else:\n",
    "        ontime_reporting_montly = pd.read_csv(path_import + \"ONTIME_REPORTING_\" + str(i) + \".csv\", usecols=cols_ontime_reporting)\n",
    "        ontime_reporting_all = pd.concat([ontime_reporting_all, ontime_reporting_montly])\n",
    "        print(ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading aircraft inventory data and merging with on-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of aircraft_inventory_data:  (7383, 3)\n",
      "Shape of ontime_reporting_all before merge:  (7422037, 13)\n",
      "Shape of ontime_reporting_all after merge:  (8478673, 15)\n"
     ]
    }
   ],
   "source": [
    "#Loading aircraft inventory list and merging it with ontime_reporting_all\n",
    "#Left joining inventory data on on-time reporting data, this would mean that some aircrafts could have missing values\n",
    "#These values could be imputed with the mean, it would introduce some noise\n",
    "aircraft_inventory_data = pd.read_csv(path_import + \"T_F41SCHEDULE_B43.csv\", encoding='latin1', usecols=cols_aircraft_inventory)\n",
    "print(\"Shape of aircraft_inventory_data: \", aircraft_inventory_data.shape)\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)\n",
    "ontime_reporting_all = ontime_reporting_all.merge(aircraft_inventory_data, on='TAIL_NUM', how=\"inner\")\n",
    "print(\"Shape of ontime_reporting_all after merge: \", ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading airport and weather data then merging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of airport_list_data:  (97, 2)\n",
      "Shape of airport_weather_data:  (38675, 7)\n"
     ]
    }
   ],
   "source": [
    "#Loading airport_list and airport_weather and meging it with ontime_reporting_all\n",
    "airport_list_data = pd.read_csv(path_import + \"airports_list.csv\", usecols=cols_airport_list)\n",
    "print(\"Shape of airport_list_data: \", airport_list_data.shape)\n",
    "airport_weather_data = pd.read_csv(path_import + \"airport_weather_2019.csv\", usecols=cols_airport_weather)\n",
    "print(\"Shape of airport_weather_data: \", airport_weather_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting DATE to datetime dtype and extracting Month and Day for merging with ontime_reporting_all\n",
    "airport_weather_data['DATE'] = pd.to_datetime(airport_weather_data['DATE'])\n",
    "airport_weather_data['MONTH'] = pd.DatetimeIndex(airport_weather_data['DATE']).month\n",
    "airport_weather_data['DAY_OF_MONTH'] = pd.DatetimeIndex(airport_weather_data['DATE']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of airport_weather_data before merge:  (38675, 9)\n",
      "Shape of airport_weather_data after merge:  (35024, 10)\n"
     ]
    }
   ],
   "source": [
    "#Merging airport_list with airport_weather for linking ORIGIN_AIRPORT_ID to NAME\n",
    "#Inner joining weather data on airportlist\n",
    "print(\"Shape of airport_weather_data before merge: \", airport_weather_data.shape)\n",
    "airport_weather_data = airport_list_data.merge(airport_weather_data, on=\"NAME\", how=\"inner\")\n",
    "print(\"Shape of airport_weather_data after merge: \", airport_weather_data.shape)\n",
    "\n",
    "#Dropping redundant columns\n",
    "airport_weather_data = airport_weather_data.drop(columns=[\"NAME\", \"DATE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Weather and on-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ontime_reporting_all before merge:  (8478673, 15)\n",
      "Shape of ontime_reporting_all after merge:  (7732471, 20)\n"
     ]
    }
   ],
   "source": [
    "#Merging airport_weather_data with ontime_reporting_all\n",
    "#Inner joining weather data on on-time reporting, inner join was chosen because it is not representative to impute missing weather data\n",
    "#This would only introduce noise as location would play a major role in te weather and imputation does not account for this\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)\n",
    "ontime_reporting_all = ontime_reporting_all.merge(airport_weather_data, how='inner', on=['ORIGIN_AIRPORT_ID', 'MONTH', 'DAY_OF_MONTH'])\n",
    "print(\"Shape of ontime_reporting_all after merge: \", ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and merging US city and on-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of us_cities_data:  (2891, 4)\n"
     ]
    }
   ],
   "source": [
    "#Loading US Cities data using predefined columns\n",
    "us_cities_data = pd.read_csv(path_import + \"us-cities-demographics-2015.csv\", usecols=cols_us_cities, delimiter=\";\")\n",
    "print(\"Shape of us_cities_data: \", us_cities_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefining US cities column names to match City with ORIGIN_CITY_NAME from on-time reporting data\n",
    "us_cities_data.rename(columns = {\"City\":\"ORIGIN_CITY_NAME\", \"Median Age\":\"MEDIAN_AGE\", \"Total Population\":\"TOT_POP\", \"Average Household Size\":\"AVG_HOUSEHOLD_SIZE\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of us_cities_data:  (567, 4)\n"
     ]
    }
   ],
   "source": [
    "#Removing duplicates as the demograhics are devided into race with a seperate count but general statistics are the same but just repeating per city\n",
    "us_cities_data = us_cities_data.drop_duplicates(subset='ORIGIN_CITY_NAME')\n",
    "print(\"Shape of us_cities_data: \", us_cities_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting City name and state from on-time reporting data and dropping state abbreviation\n",
    "ontime_reporting_all[\"ORIGIN_CITY_NAME\"] = ontime_reporting_all['ORIGIN_CITY_NAME'].str.split(',').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ontime_reporting_all before merge:  (7732471, 20)\n",
      "Shape of ontime_reporting_all before merge:  (6959322, 23)\n"
     ]
    }
   ],
   "source": [
    "#Merging on-time reporting with US cities data via an inner join,\n",
    "#The inner join has been chosen because it is difficult to impute the missing values for each city when doing a left join and would only generate noise and a skewed image\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)\n",
    "ontime_reporting_all = ontime_reporting_all.merge(us_cities_data, how='inner', on='ORIGIN_CITY_NAME')\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and merging ORIGIN and DESTINATION longitude/latitude with On-Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of airport_geolocation_data:  (8970, 4)\n",
      "Shape of airport_geolocation_data:  (1994, 3)\n"
     ]
    }
   ],
   "source": [
    "airport_geolocation_data = pd.read_csv(path_import + \"airports_geolocation_coordinates.csv\", usecols=cols_airport_geolocation)\n",
    "print(\"Shape of airport_geolocation_data: \", airport_geolocation_data.shape)\n",
    "#Dropping all entries exept US airports then dropping the country_code column\n",
    "airport_geolocation_data = airport_geolocation_data.loc[(airport_geolocation_data[\"country_code\"] == \"US\")]\n",
    "airport_geolocation_data = airport_geolocation_data.drop(columns=[\"country_code\"])\n",
    "print(\"Shape of airport_geolocation_data: \", airport_geolocation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ontime_reporting_all before merge:  (6959322, 23)\n",
      "Shape of ontime_reporting_all after merge:  (6904014, 24)\n"
     ]
    }
   ],
   "source": [
    "#Merging latitude and longitude on the ORIGIN column, then renaming them and dropping redundant columns\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)\n",
    "ontime_reporting_all = ontime_reporting_all.merge(airport_geolocation_data, how='inner', left_on=\"ORIGIN\", right_on='iata')\n",
    "ontime_reporting_all = ontime_reporting_all.rename(columns = {\"latitude\":\"ORIGIN_LAT\",\"longitude\":\"ORIGIN_LONG\"})\n",
    "ontime_reporting_all = ontime_reporting_all.drop(columns=[\"iata\", \"ORIGIN\"])\n",
    "print(\"Shape of ontime_reporting_all after merge: \", ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ontime_reporting_all before merge:  (6904014, 24)\n",
      "Shape of ontime_reporting_all after merge:  (6808079, 25)\n"
     ]
    }
   ],
   "source": [
    "#Merging latitude and longitude on the DEST column, then renaming them and dropping redundant columns\n",
    "print(\"Shape of ontime_reporting_all before merge: \", ontime_reporting_all.shape)\n",
    "ontime_reporting_all = ontime_reporting_all.merge(airport_geolocation_data, how='inner', left_on=\"DEST\", right_on='iata')\n",
    "ontime_reporting_all = ontime_reporting_all.rename(columns = {\"latitude\":\"DEST_LAT\",\"longitude\":\"DEST_LONG\"})\n",
    "ontime_reporting_all = ontime_reporting_all.drop(columns=[\"iata\", \"DEST\"])\n",
    "print(\"Shape of ontime_reporting_all after merge: \", ontime_reporting_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the whole dataset\n",
    "ontime_reporting_all.to_csv(path_export + \"ontime_reporting_export.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
