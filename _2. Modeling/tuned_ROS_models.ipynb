{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "random_state = 101\n",
    "path_csv = \"/content/drive/MyDrive/Thesis Data/processed_data/Scaled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/yus002/logistic-regression-optuna-tuning\n",
    "#Code used from the above author\n",
    "#Helps to reduce size in memory of the data so that models can run faster\n",
    "\n",
    "def rm(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "X_train = pd.read_csv(path_csv + \"ontime_reporting_X_train.csv\")\n",
    "y_train = pd.read_csv(path_csv + \"ontime_reporting_y_train.csv\")\n",
    "\n",
    "X_train = rm(X_train)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Logistic Regression model on ROS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(trial):\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 0.001, 1000.0, log=True), #log=True helps to search a large range efficiently with fewer trials\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"sag\",\"saga\",\"newton-cholesky\"]), #\"sag\" and \"saga\" works on features with the same scale\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "    }\n",
    "\n",
    "    steps = [\n",
    "        (\"ros\", RandomOverSampler(sampling_strategy=\"minority\", random_state=random_state)),\n",
    "        (\"logreg\", LogisticRegression(**params, random_state=random_state, n_jobs=-1, verbose=2))\n",
    "    ]\n",
    "\n",
    "    xgb_pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "\n",
    "    scores = cross_val_score(xgb_pipeline, X_train, y_train, scoring=\"roc_auc\", n_jobs=-1, cv=skfold)\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', study_name='Logistic Regression Tuned ROS', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_lr, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best_params = study.best_params\n",
    "#{'C': 0.012341115473953784, 'solver': 'saga', 'penalty': None}\n",
    "print(logreg_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lr_model = LogisticRegression(**logreg_best_params, random_state=random_state, n_jobs=-1, verbose=2)\n",
    "tuned_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_lr_model, 'lr_tuned_ROS.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned XGBoost model on ROS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/55591063/how-to-perform-smote-with-cross-validation-in-sklearn-in-python\n",
    "\n",
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 150, 350, step=50),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-2, 1e-2, 15e-2, 1e-1, 2e-1]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.0, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.8, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 20),\n",
    "    }\n",
    "\n",
    "    steps = [\n",
    "        (\"ros\", RandomOverSampler(sampling_strategy=\"minority\", random_state=random_state)),\n",
    "        (\"xgb\", xgb.XGBClassifier(**params, random_state=random_state, n_jobs=-1, verbosity=1, device=\"cuda\"))\n",
    "    ]\n",
    "\n",
    "    xgb_pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "\n",
    "    scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=skfold, scoring=\"roc_auc\")\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', study_name='XGBoost Tuned ROS', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_xgboost, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_best_params = study.best_params\n",
    "#{'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 16, 'subsample': 0.8180174838276538, 'colsample_bytree': 0.8949014373055646, 'min_child_weight': 17}\n",
    "print(xgboost_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb_imbalanced = xgb.XGBClassifier(**xgboost_best_params, random_state=random_state, n_jobs=-1, verbosity=2, device=\"cuda\")\n",
    "tuned_xgb_imbalanced.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_xgb_imbalanced, 'xgb_tuned_ROS.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Random Forests model on ROS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned TabNet model on ROS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
