{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install optuna\n",
    "! pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import optuna\n",
    "import torch\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "random_state = 101\n",
    "path_csv = \"/content/drive/MyDrive/Thesis Data/processed_data/Scaled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/yus002/logistic-regression-optuna-tuning\n",
    "#Code used from the above author\n",
    "#Helps to reduce size in memory of the data so that models can run faster\n",
    "\n",
    "def rm(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "X_train = pd.read_csv(path_csv + \"ontime_reporting_X_train.csv\")\n",
    "y_train = pd.read_csv(path_csv + \"ontime_reporting_y_train.csv\")\n",
    "\n",
    "X_train = rm(X_train)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Logistic Regression model on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://emerginginvestigators.org/articles/comparison-of-the-ease-of-use-and-accuracy-of-two-machine-learning-algorithms-forestry-case-study/pdf#:~:text=The%20key%20difference%20between%20newton,to%20newton%2Dcg%20and%20lbfgs.\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#Also paper by Dahir\n",
    "\n",
    "def objective_lr(trial):\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 0.001, 1000.0, log=True), #log=True helps to search a large range efficiently with fewer trials\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"sag\",\"saga\",\"newton-cholesky\"]), #\"sag\" and \"saga\" works on features with the same scale\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\", None])\n",
    "    }\n",
    "\n",
    "    model_lrClass = LogisticRegression(**params, random_state=random_state, n_jobs=-1, verbose=2)\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "\n",
    "    scores = cross_val_score(model_lrClass, X_train, y_train, scoring=\"roc_auc\", n_jobs=-1, cv=skfold)\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', study_name='Logistic Regression Tuned Imbalanced', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_lr, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.012341115473953784, 'solver': 'saga', 'penalty': None}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params #{'C': 0.012341115473953784, 'solver': 'saga', 'penalty': None}\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Tuned models on imbalanced data/Logistic Regression/Optuna trial optimization history.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lr_model = LogisticRegression(**best_params, random_state=random_state, n_jobs=-1, verbose=2)\n",
    "tuned_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_lr_model, 'lr_tuned_imbalance.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned XGBoost model on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 150, 350, step=50),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-2, 1e-2, 15e-2, 1e-1, 2e-1]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.0, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.8, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 20),\n",
    "    }\n",
    "\n",
    "    model_xgbClass = xgb.XGBClassifier(**params, random_state=random_state, n_jobs=-1, verbosity=2, device=\"cuda\")\n",
    "    \n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_xgbClass, X_train, y_train, cv=skfold, scoring=\"roc_auc\")  \n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', study_name='XGBoost Tuned Imbalanced', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_xgboost, n_trials=20, show_progress_bar=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 350, 'learning_rate': 0.05, 'max_depth': 16, 'subsample': 0.9951754722614875, 'colsample_bytree': 0.8668977611468552, 'min_child_weight': 8}\n"
     ]
    }
   ],
   "source": [
    "xgboost_best_params = study.best_params #{'n_estimators': 350, 'learning_rate': 0.05, 'max_depth': 16, 'subsample': 0.9951754722614875, 'colsample_bytree': 0.8668977611468552, 'min_child_weight': 8}\n",
    "print(xgboost_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Tuned models on imbalanced data/XGBoost/Optuna trial optimaization history.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_xgb_imbalanced = xgb.XGBClassifier(**xgboost_best_params, random_state=random_state, n_jobs=-1, verbosity=2, device=\"cuda\") \n",
    "tuned_xgb_imbalanced.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_xgb_imbalanced, '/content/drive/MyDrive/Thesis data/xgb_tuned_imbalance.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Random Forests model on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 9),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 150, 350, step=50)\n",
    "    }\n",
    "\n",
    "    model_rfClass = RandomForestClassifier(**params, random_state=random_state, verbose=1, n_jobs=-1)\n",
    "        \n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_rfClass, X_train, y_train, cv=skfold, scoring=\"roc_auc\")  \n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', study_name='Random Forests Tuned Imbalanced', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_xgboost, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 6, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "rf_best_params = study.best_params #{'max_depth': 20, 'min_samples_leaf': 5, 'min_samples_split': 6, 'n_estimators': 250}\n",
    "print(rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Tuned models on imbalanced data/Random Forests/Optuna trial optimization history.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf_imbalanced = RandomForestClassifier(**rf_best_params, random_state=random_state, verbose=1, n_jobs=-1)\n",
    "tuned_rf_imbalanced.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tuned_rf_imbalanced, 'rf_tuned_imbalance.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned TabNet model on imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_tabnet(trial):\n",
    "\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 8,  32, step=4),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 2, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0, step=0.01),\n",
    "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
    "        \"lambda_sparse\": trial.suggest_categorical(\"lambda_sparse\", [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]),\n",
    "    }\n",
    "\n",
    "    model_tabnetClass = TabNetClassifier(**params, seed=random_state, verbose=2, optimizer_params=dict(lr=2e-2, weight_decay=1e-5), n_a=params[\"n_d\"], device_name=\"cuda\")\n",
    "\n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, val_index in skfold.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "\n",
    "\n",
    "        model_tabnetClass.fit(X_train = X_train_fold.values,\n",
    "                              y_train = y_train_fold,\n",
    "                              eval_set = [(X_val_fold.values, y_val_fold)],\n",
    "                              patience=2,\n",
    "                              max_epochs=4,\n",
    "                              batch_size=14000,\n",
    "                              virtual_batch_size=14000,\n",
    "                              eval_metric=[\"auc\"]\n",
    "                              )\n",
    "\n",
    "        y_pred = model_tabnetClass.predict(X_val_fold.values)\n",
    "\n",
    "        auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n",
    "\n",
    "    return np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name='TabNet Tuned Imbalanced', sampler=TPESampler(seed=random_state))\n",
    "study.optimize(objective_tabnet, n_trials=20, show_progress_bar=True, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_d': 64, 'n_steps': 4, 'gamma': 1.78, 'n_shared': 1, 'lambda_sparse': 0.001}\n"
     ]
    }
   ],
   "source": [
    "tabnet_best_params = study.best_params #{'n_d': 64, 'n_steps': 4, 'gamma': 1.78, 'n_shared': 1, 'lambda_sparse': 0.001}\n",
    "print(tabnet_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.matplotlib.plot_optimization_history(study) #Optuna trial optimaization history.png\n",
    "visualization.matplotlib.plot_param_importances(study) #Optuna hyperparameter importance.png\n",
    "visualization.plot_slice(study) #Optuna hyperparameter slice plot.png\n",
    "visualization.matplotlib.plot_timeline(study) #Optuna trial timeline.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Tuned models on imbalanced data/TabNet/Optuna trial optimization history.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_tuned_imbalance = TabNetClassifier(**tabnet_best_params, seed=random_state, verbose=2, optimizer_params=dict(lr=2e-2, weight_decay=1e-5), n_a=tabnet_best_params[\"n_d\"], device_name=\"cuda\")\n",
    "tabnet_tuned_imbalance.fit(X_train.values, y_train, max_epochs=4, batch_size=14000, virtual_batch_size=14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tabnet_tuned_imbalance, 'TabNet_tuned_imbalance.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
