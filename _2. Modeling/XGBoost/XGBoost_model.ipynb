{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "random_state = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://forecastegy.com/posts/xgboost-hyperparameter-tuning-with-optuna/\n",
    "#https://aetperf.github.io/2021/02/16/Optuna-+-XGBoost-on-a-tabular-dataset.html\n",
    "#+ ChatGPT for debugging\n",
    "\n",
    "# def objective_xgboost(trial):\n",
    "#     params = {\n",
    "#         \"n_estimators\": trial.suggest_int(\"max_depth\", 100, 1000, step=50),\n",
    "#         \"verbosity\": 3,\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 1, 15),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#         \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "#         \"enable_categorical\": True\n",
    "#     }\n",
    "\n",
    "#     model_xgbClass = xgb.XGBClassifier(**params, random_state=random_state, n_jobs=-1, tree_method=\"gpu_hist\") #enable_categorical=True\n",
    "    \n",
    "#     skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "#     auc_scores = []\n",
    "\n",
    "#     for train_index, val_index in skfold.split(X_train, y_train):\n",
    "#         X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "#         X_val_fold, y_val_fold = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "#         model_xgbClass.fit(X_train_fold, y_train_fold, \n",
    "#                            eval_set = [(X_val_fold, y_val_fold)],\n",
    "#                            early_stopping_rounds = 5,\n",
    "#                            verbose = 3\n",
    "#                            )\n",
    "#         y_pred = model_xgbClass.predict(X_val_fold)#[:, 1]\n",
    "#         auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n",
    "\n",
    "#     return np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"max_depth\", 100, 1000, step=50),\n",
    "        \"verbosity\": 3,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        \"enable_categorical\": True\n",
    "    }\n",
    "\n",
    "    model_xgbClass = xgb.XGBClassifier(**params, random_state=random_state, n_jobs=-1, tree_method=\"gpu_hist\") #enable_categorical=True\n",
    "    \n",
    "    skfold = StratifiedKFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_xgbClass, X_train, y_train, cv=skfold, scoring=\"roc_auc\")  \n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_xgboost, n_trials=1) #Adjust trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "final_model = xgb.XGBClassifier(**best_params, random_state=random_state, n_jobs=-1, enable_categorical=True)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "test_auroc = roc_auc_score(y_test, y_pred)\n",
    "print(test_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save_model('xgboost_model.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
